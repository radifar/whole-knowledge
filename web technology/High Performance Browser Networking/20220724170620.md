---
title: "Networking 101, chapter 1 - Primer on Latency and Bandwidth"
date: 2022-07-24
author: "Muhammad Radifar"
id: 20220724170626
---

# Networking 101, chapter 1 - Primer on Latency and Bandwidth

Summary of
High Performance Browser Networking by Ilya Grigorik

#book #summary #networking #userexperience #networkspeed

## Speed Is a Feature

Faster sites ⇒ Better user engagement, better user retention, higher conversions.

2 critical components that dictate the performance of all network traffic: **latency and bandwidth**.

Latency → Time from the source sending a packet to the destination receiving it
Bandwidth → Maximum throughput of a logical or physical communication path

## The Many Components of Latency

- Every system contains multiple sources, or components, contributing to the overall time it takes for a message to be delivered.

1. Propagation delay - time required for a message to _travel from the sender to receiver_, which is a function of a distance over speed with which the signal propagates.
2. Transmission delay - time required to _push all the packet's bits into the link_, which is a function of the packet's length and data rate of the link.
3. Processing delay - time required to _process the packet header, check for bit-level errors, and determine the packet's destination_.
4. Queuing delay - time the packet is _waiting in the queue_ until it can be processed.

Propagation → dictated by distance and the medium through which the signal travels
Transmission → dictated by the available data rate of the transmitting link and has nothing to do with the distance between the client and the server.

Once packet arrives at the router → router must examine the packet header to determine the outgoing route and may run other checks on the data. Much of this logic is now often done in hardware, so the delays are very small, but they do exist.

Finally, if the packets are arriving at a faster rate than the router is capable of procesing → packets are queued inside an incoming buffer.

## Speed of Light and Propagation Latency

As rule of thumb → speed of light in fiber is around 200,000 km/s, which corresponds to a refractive index of ~1.5.

| Route | Distance | Time, light in vacuum | Time, light in fiber | Round-trip time (RTT) in fiber |
| --- | --- | --- | --- | --- |
| New York to San Francisco | 4,148 km | 14 ms | **21 ms** | 42 ms |
| New York to London | 5,585 km | 19 ms | **28 ms** | 56 ms |
| New York to Sydney | 15,993 km | 53 ms | **80 ms** | 160 ms |
| Equatorial circumference | 40,075 km | 133.7 ms | **200 ms** | 200 ms |Table 1-1

the numbers in [Table 1-1](https://hpbn.co/primer-on-latency-and-bandwidth/#geo-latency-table) are also unrealistically optimistic in that they assume that the packet travels over a fiber-optic cable along the great-circle path (the shortest distance between two points on the globe) between the cities. In practice, that is rarely the case, and the packet would take a much longer route between New York and Sydney. Each hop along this route will introduce additional routing, processing, queuing, and transmission delays. As a result, the actual RTT between New York and Sydney, over our existing networks, works out to be in the 200–300 millisecond range. All things considered, that still seems pretty fast, right?

Studies have shown that we can perceive 100-200 ms delay as "lag", once the 300 ms delay threshold is exceeded, the interaction is often reported as "sluggish", and at the 1 second barier, many users have already performed a _mental context switch_ while waiting for the response.

To deliver the best experience and to keep our users engaged in the task at hand, we need our applicaitons to respond within hundreds of ms.

> Note: Content delivery network (CDN) → enables us to significantly reduce the propagation time of all the data packets. We may not be able to make the packets travel faster, but we can reduce the distance by strategically positioning our servers closer to the users!

## Last-Mile Latency

Ironically, it is often the last few miles, not the crossing of oceans or continents, where significant latency is introduced: **the infamous last-mile problem**.

> Note: Latency, not bandwidth, is the performance bottleneck for most websites! To understand why, we need to understand the mechanics of TCP and HTTP protocols.

## Bandwidth at the Network Edge

However, while a high-bandwidth link to your ISP is desirable, it is also not a guarantee of end-to-end performance; just because a bandwidth test promises high data rates does not mean that you can or should expect same performance from other remote servers. The network could be congested at any intermediate node due to high demand, hardware failures, a concentrated network attack, or a host of other reasons. High variability of throughput and latency performance is an inherent property of our data networks — predicting, managing, and adapting to the continuously changing "network weather" is a complex task.

## Delivering Higher Bandwidth and Lower Latencies

Increasing bandwidth is as simple as adding more fiber links.
Improving latency, on the other hand, is a very different story. the speed of light places a hard limit on the minimum latency. Alternatively, since we can't make light travel faster, we can make the distance shorter. However, laying new cables is also not always possible due to the constraints imposed by the physical terrain, social and political reasons, and of course, the associated costs.

As a result, to improve performance of our applications, we need to architect and optimize our protocols and networking code:
- reduce round trips
- move the data closer to the client
- build applications that can hide the latency through caching, pre-fetching, and variety of similar techniques, as explained ins subsequent chapters.