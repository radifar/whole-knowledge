---
title: "Networking 101, chapter 2 - Building Blocks of TCP"
date: 2022-07-24
author: "Muhammad Radifar"
id: 20220724180508
---

# Networking 101, chapter 2 - Building Blocks of TCP

Summary of
High Performance Browser Networking by Ilya Grigorik

#book #summary #networking #tcp

IP → Internet Protocol → provides host-to-host routing and addressing
TCP → Transmission Control Protocol → provides the abstraction of a reliable network running over an unreliable channel.

TCP/IP is also commonly referred to as the Internet Protocol Suite and was first proposed by Vint Cerf and Bob Kahn in their 1974 paper title "A Protocol for Packet Network Intercommunication"

The original proposal (RFC 675) was revised several times, and in 1981 the v4 specification of TCP/IP was published not as one, but as two separatae RFCs:
- RFC 791 – Internet Protocol
- RFC 793 – Transmission Control Protocol

TCP provides an effective abstraction of a reliable network running over an unreliable channel, hiding most of the complexity of network communication from our applications:
- retransmission of lost data
- in-order delivery
- congestion control and avoidance
- data integrity, and more.

TCP **guarantees** that all bytes sent will be **identical** with bytes received and that they will arrive in the **same order.** TCP is optimized for accurate delivery, rather than a timely one.

HTTP standard does not mandate TCP as the only transport protocol. If we wanted, we could deliver HTTP via a datagram socket (User Datagram Protocol or UDP), or any other transport protocol of our choice, but in practice all HTTP traffic on the Internet today is delivered via TCP due to the many great and convenient features it provides out of the box.

Because of this, understanding some of the core mechanism of TCP is essential knowledge for building an optimized web experience.

## Three-Way handshake

before application data exchange → must agree the packet sequence numbers, as well as a number of other connection specific variables.
SYN → SYN ACK → ACK

The server must wait for the ACK before it can dispatch any data. Each new connection will have a full roundtrip of latency before any application data can be transferred.

The delay imposed by the three-way handshake makes new TCP connections expensive to create, and is one of the big reasons why **connection reuse is a critical optimization** for any application running over TCP.

## Congestion Avoidance and Control

John Nagle (1984) describe "congestion collapse" → affect any network with asymmetric bandwidth capacity between the network nodes.
> Hosts are sending each packet several times, and eventually some copy of each packet arrives at its destination. This is congestion collapse. 

To address these issues, multiple mechanisms were implemented in TCP to govern the rate with which the data can be sent in both directions: flow control, congestion control, and congestion avoidance.

### Flow Control

→ Mechanism to prevent the sender from overwhelming the receiver with data it may not be able to process. → each side of the TCP connection advertises its own **receive window (rwnd)**, which communicates the size of the available buffer space to hold the incoming data.

> Note: implemented with **window scaling**, however intermediate nodes, routers, and firewalls can rewrite or even strip this option entirely.

### Slow-start

To prevent sender/receiver saturate each other and can adapt to any bandwidth (because both have no idea how much bandwidth available on the other side).

> Note: First server estimate the available capacity between the client and the server by exchanging data, implemented with **Congestion window size (cwnd)** → Sender-side limit the amount of data the sender can have in flight before receiving an acknowledgment (ACK) from the client. Then grow the window size as the packets are acknowledged.

Slow-start is not as big of an issue for large, streaming downloads, as the client and the server will arrive at their maximum window sizes after few hundred ms and continue to transmit at near maximum speeds – the cost of the slow-start phase is amortized over lifetime of the larger transfer.

However, for many HTTP connections, which are often short and bursty, it is not unusual for the data transfer to finish before the maximum window size is reached. As a result, the performance of many web applications is often limited by the roundtrip time between server and client → affect the performance of small transfers.

> Note: Slow-start Restart → resets the congestion window after idle

### Congestion Avoidance

It is important to recognize that TCP Is specifically designed to use **packet loss** as a **feedback mechanism** to help regulate its performance.

Slow-start initialize → set conservative window
for every roundtrip → doubles the amount of data flight **until** it exceeds the receiver's flow-control window, a system-configured congestion threshold (sstresh) window, or **until a packet is lost**, at which point **congestion avoidance algorithm** takes over.

> Note: Proportional Rate Reduction (PRR) the default congestion-avoidance algorithm (2015) → available in Linux 3.2+ kernels – another good reason to upgrade your servers!

## Bandwidth-Delay Product
→ Product of data link’s capacity and its end-to-end delay. The result is the maximum amount of unacknowledged data that can be in flight at any point in time.

The built-in congestion control and congestion avoidance mechanisms in TCP carry another important performance implication: the optimal sender and receiver window sizes must vary **based on the roundtrip time** and the **target data rate** between them.

If either the sender or receiver are frequently forced to stop and wait for ACKs for previous packets, then this would create gaps in the data flow, which would consequently **limit the maximum throughput of the connection**.

If you have ever wondered why your connection is transmitting at a fraction of the available bandwidth, even when you know that both the client and the server are capable of higher rates, then it is likely due to a small window size: a saturated peer advertising low receive window, bad network weather and high packet loss resetting the congestion window, or explicit traffic shaping that could have been applied to limit throughput of your connection.

## Head-of-Line Blocking

recall that every TCP packet carries a unique sequence number when put on the wire, and the data must be passed to the receiver in-order. If one of the packets is lost en route to the receiver, then all subsequent packets must be held in the receiver’s TCP buffer until the lost packet is retransmitted and arrives at the receiver. Because this work is done within the TCP layer, **our application has no visibility into the TCP retransmissions or the queued packet buffers, and must wait for the full sequence before it is able to access the data**. Instead, it simply sees a **delivery delay when it tries to read the data from the socket**. This effect is known as **TCP head-of-line (HOL) blocking**.

## Optimizing for TCP

The best way to optimize TCP is to tune how TCP senses the current network conditions and **adapts its behavior based on the type and the requirements of the layers below and above it**: wireless networks may need different congestion algorithms, and some applications may need custom quality of sevice (QoS) semantics to deliver the best experience.

### Tuning Server Configuration

simply upgrading your hosts to their latest system versions. TCP best practices and underlying algorithms that govern its performance continue to evolve, and most of these changes are available only in the latest kernels. In short, keep your servers up to date to ensure the optimal interaction between the sender’s and receiver’s TCP stacks.

With the latest kernel in place, it is good practice to ensure that your server is configured to use the following best practices:

[Increasing TCP’s Initial Congestion Window](https://hpbn.co/building-blocks-of-tcp/#increasing-tcps-initial-congestion-window)
> A larger starting congestion window allows TCP to transfer more data in the first roundtrip and significantly accelerates the window growth.

[Slow-Start Restart](https://hpbn.co/building-blocks-of-tcp/#slow-start-restart)
> Disabling slow-start after idle will improve performance of long-lived TCP connections that transfer data in periodic bursts.

[Window Scaling (RFC 1323)](https://hpbn.co/building-blocks-of-tcp/#window-scaling-rfc-1323)
> Enabling window scaling increases the maximum receive window size and allows high-latency connections to achieve better throughput.

[TCP Fast Open](https://hpbn.co/building-blocks-of-tcp/#tcp-fast-open)
> Allows application data to be sent in the initial SYN packet in certain situations. TFO is a new optimization, which requires support both on client and server; investigate if your application can make use of it.

### Tuning Application Behavior

- Eliminating unnecessary data transfers is, of course, the single best optimization — e.g., eliminating unnecessary resources or ensuring that the minimum number of bits is transferred by applying the appropriate compression algorithm.
- Locating the bits closer to the client, by geo-distributing servers around the world — e.g., using a CDN — will help reduce latency of network roundtrips and significantly improve TCP performance.
- Where possible, existing TCP connections should be reused to minimize overhead imposed by slow-start and other congestion mechanisms.

### Performance Checklist

* Upgrade server kernel to latest version.
* Ensure that cwnd size is set to 10.
* Ensure that window scaling is enabled.
* Disable slow-start after idle.
* Investigate enabling TCP Fast Open.
* Eliminate redundant data transfers.
* Compress transferred data.
* Position servers closer to the user to reduce roundtrip times.
* Reuse TCP connections whenever possible.
* Investigate ["TCP Tuning for HTTP"](https://hpbn.co/http-tcp) recommendations.
